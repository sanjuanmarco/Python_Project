{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Nov  4 12:00:49 2017\n",
    "\n",
    "@author: NishitP\n",
    "\"\"\"\n",
    "#import os\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import seaborn as sb\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "import nltk.corpus \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import svm\n",
    "#before reading the files, setup the working directory to point to project repo\n",
    "#reading data files \n",
    "\n",
    "\n",
    "test_filename = 'test.csv'\n",
    "train_filename = 'train.csv'\n",
    "valid_filename = 'valid.csv'\n",
    "\n",
    "train_news = pd.read_csv(train_filename)\n",
    "test_news = pd.read_csv(test_filename)\n",
    "valid_news = pd.read_csv(valid_filename)\n",
    "\n",
    "\n",
    "\n",
    "#data observation\n",
    "def data_obs():\n",
    "    print(\"training dataset size:\")\n",
    "    print(train_news.shape)\n",
    "    print(train_news.head(10))\n",
    "\n",
    "    #below dataset were used for testing and validation purposes\n",
    "    print(test_news.shape)\n",
    "    print(test_news.head(10))\n",
    "    \n",
    "    print(valid_news.shape)\n",
    "    print(valid_news.head(10))\n",
    "\n",
    "#check the data by calling below function\n",
    "#data_obs()\n",
    "\n",
    "#distribution of classes for prediction\n",
    "def create_distribution(dataFile):\n",
    "    \n",
    "    return sb.countplot(x='Label', data=dataFile, palette='hls')\n",
    "    \n",
    "\n",
    "#by calling below we can see that training, test and valid data seems to be failry evenly distributed between the classes\n",
    "create_distribution(train_news)\n",
    "create_distribution(test_news)\n",
    "create_distribution(valid_news)\n",
    "\n",
    "\n",
    "#data integrity check (missing label values)\n",
    "#none of the datasets contains missing values therefore no cleaning required\n",
    "def data_qualityCheck():\n",
    "    \n",
    "    print(\"Checking data qualitites...\")\n",
    "    train_news.isnull().sum()\n",
    "    train_news.info()\n",
    "        \n",
    "    print(\"check finished.\")\n",
    "\n",
    "    #below datasets were used to \n",
    "    test_news.isnull().sum()\n",
    "    test_news.info()\n",
    "\n",
    "    valid_news.isnull().sum()\n",
    "    valid_news.info()\n",
    "\n",
    "#run the below function call to see the quality check results\n",
    "#data_qualityCheck()\n",
    "\n",
    "\n",
    "\n",
    "#eng_stemmer = SnowballStemmer('english')\n",
    "#stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "#Stemming\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for token in tokens:\n",
    "        stemmed.append(stemmer.stem(token))\n",
    "    return stemmed\n",
    "\n",
    "#process the data\n",
    "def process_data(data,exclude_stopword=True,stem=True):\n",
    "    tokens = [w.lower() for w in data]\n",
    "    tokens_stemmed = tokens\n",
    "    tokens_stemmed = stem_tokens(tokens, eng_stemmer)\n",
    "    tokens_stemmed = [w for w in tokens_stemmed if w not in stopwords ]\n",
    "    return tokens_stemmed\n",
    "\n",
    "\n",
    "#creating ngrams\n",
    "#unigram \n",
    "def create_unigram(words):\n",
    "    assert type(words) == list\n",
    "    return words\n",
    "\n",
    "#bigram\n",
    "def create_bigrams(words):\n",
    "    assert type(words) == list\n",
    "    skip = 0\n",
    "    join_str = \" \"\n",
    "    Len = len(words)\n",
    "    if Len > 1:\n",
    "        lst = []\n",
    "        for i in range(Len-1):\n",
    "            for k in range(1,skip+2):\n",
    "                if i+k < Len:\n",
    "                    lst.append(join_str.join([words[i],words[i+k]]))\n",
    "    else:\n",
    "        #set it as unigram\n",
    "        lst = create_unigram(words)\n",
    "    return lst\n",
    "\n",
    "\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer()\n",
      "  (0, 9676)\t1\n",
      "  (0, 10988)\t1\n",
      "  (0, 1044)\t1\n",
      "  (0, 6639)\t1\n",
      "  (0, 8376)\t1\n",
      "  (0, 5115)\t1\n",
      "  (0, 10709)\t1\n",
      "  (0, 11036)\t1\n",
      "  (0, 11296)\t1\n",
      "  (0, 615)\t1\n",
      "  (0, 7728)\t1\n",
      "  (0, 3278)\t1\n",
      "  (1, 10988)\t1\n",
      "  (1, 11934)\t2\n",
      "  (1, 3434)\t1\n",
      "  (1, 3185)\t1\n",
      "  (1, 7672)\t1\n",
      "  (1, 2475)\t1\n",
      "  (1, 10425)\t1\n",
      "  (1, 6052)\t1\n",
      "  (1, 10426)\t2\n",
      "  (1, 7418)\t1\n",
      "  (1, 4860)\t1\n",
      "  (1, 11138)\t1\n",
      "  (1, 7674)\t1\n",
      "  :\t:\n",
      "  (10239, 10988)\t1\n",
      "  (10239, 7672)\t2\n",
      "  (10239, 11110)\t2\n",
      "  (10239, 5267)\t1\n",
      "  (10239, 7828)\t1\n",
      "  (10239, 7824)\t1\n",
      "  (10239, 1159)\t1\n",
      "  (10239, 12151)\t2\n",
      "  (10239, 6327)\t1\n",
      "  (10239, 6603)\t1\n",
      "  (10239, 11013)\t1\n",
      "  (10239, 11004)\t1\n",
      "  (10239, 3309)\t1\n",
      "  (10239, 12158)\t1\n",
      "  (10239, 11660)\t2\n",
      "  (10239, 799)\t1\n",
      "  (10239, 2568)\t1\n",
      "  (10239, 11622)\t1\n",
      "  (10239, 2549)\t1\n",
      "  (10239, 10660)\t1\n",
      "  (10239, 8996)\t1\n",
      "  (10239, 10918)\t1\n",
      "  (10239, 3989)\t1\n",
      "  (10239, 10594)\t1\n",
      "  (10239, 6853)\t1\n",
      "0        Says the Annies List political group supports ...\n",
      "1        When did the decline of coal start? It started...\n",
      "2        Hillary Clinton agrees with John McCain \"by vo...\n",
      "3        Health care reform legislation is likely to ma...\n",
      "4        The economic turnaround started at the end of ...\n",
      "                               ...                        \n",
      "10235    There are a larger number of shark attacks in ...\n",
      "10236    Democrats have now become the party of the [At...\n",
      "10237    Says an alternative to Social Security that op...\n",
      "10238    On lifting the U.S. Cuban embargo and allowing...\n",
      "10239    The Department of Veterans Affairs has a manua...\n",
      "Name: Statement, Length: 10240, dtype: object\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Vectorizer/glove.6B.50d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4260/4105288144.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;31m#Using Word2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Vectorizer/glove.6B.50d.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m     w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))\n\u001b[0;32m     80\u001b[0m            for line in lines}\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Vectorizer/glove.6B.50d.txt'"
     ]
    }
   ],
   "source": [
    "countV = CountVectorizer()\n",
    "train_count = countV.fit_transform(train_news['Statement'].values)\n",
    "\n",
    "print(countV)\n",
    "print(train_count)\n",
    "\n",
    "#print training doc term matrix\n",
    "#we have matrix of size of (10240, 12196) by calling below\n",
    "def get_countVectorizer_stats():\n",
    "    \n",
    "    #vocab size\n",
    "    train_count.shape\n",
    "\n",
    "    #check vocabulary using below command\n",
    "    print(countV.vocabulary_)\n",
    "\n",
    "    #get feature names\n",
    "    print(countV.get_feature_names()[:25])\n",
    "\n",
    "\n",
    "#create tf-df frequency features\n",
    "#tf-idf \n",
    "tfidfV = TfidfTransformer()\n",
    "train_tfidf = tfidfV.fit_transform(train_count)\n",
    "\n",
    "def get_tfidf_stats():\n",
    "    train_tfidf.shape\n",
    "    #get train data feature names \n",
    "    print(train_tfidf.A[:10])\n",
    "\n",
    "\n",
    "#bag of words - with n-grams\n",
    "#countV_ngram = CountVectorizer(ngram_range=(1,3),stop_words='english')\n",
    "#tfidf_ngram  = TfidfTransformer(use_idf=True,smooth_idf=True)\n",
    "\n",
    "tfidf_ngram = TfidfVectorizer(stop_words='english',ngram_range=(1,4),use_idf=True,smooth_idf=True)\n",
    "\n",
    "\n",
    "#POS Tagging\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    "\n",
    "cutoff = int(.75 * len(tagged_sentences))\n",
    "training_sentences = train_news['Statement']\n",
    " \n",
    "print(training_sentences)\n",
    "\n",
    "#training POS tagger based on words\n",
    "def features(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }\n",
    "    \n",
    "    \n",
    "#helper function to strip tags from tagged corpus\t\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]\n",
    "\n",
    "\n",
    "\n",
    "#Using Word2Vec \n",
    "filepath = \"C:\\Users\\jake\\Documents\\GitHub\\Vectorizer\"\n",
    "with open('Vectorizer/glove.6B.50d.txt', \"rb\") as lines:\n",
    "    w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "           for line in lines}\n",
    "\n",
    "\n",
    "\n",
    "#model = gensim.models.Word2Vec(X, size=100) # x be tokenized text\n",
    "#w2v = dict(zip(model.wv.index2word, model.wv.syn0))\n",
    "\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6170129361034888"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_pipeline_ngram = Pipeline([\n",
    "        ('svm_tfidf',tfidf_ngram),\n",
    "        ('svm_clf',svm.LinearSVC())\n",
    "        ])\n",
    "\n",
    "svm_pipeline_ngram.fit(train_news['Statement'],train_news['Label'])\n",
    "predicted_svm_ngram = svm_pipeline_ngram.predict(test_news['Statement'])\n",
    "np.mean(predicted_svm_ngram == test_news['Label'])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
